{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GJ3c3Epn8-7dNGxcp-9qj3iuMMbRu4Jv",
      "authorship_tag": "ABX9TyO7jSNQGuxA9//f/Cuoy3qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/honyango/test/blob/main/DATA_INGESTION_CLUSTER2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INSTALL DEPENDENCIES"
      ],
      "metadata": {
        "id": "uJrGQH90EIPX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URuwUIqyDzyW",
        "outputId": "698090fb-c557-4184-f95b-3bf1006cf2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Spark Version: 3.5.0\n",
            "Hadoop Version (simulated): 3.3.0\n"
          ]
        }
      ],
      "source": [
        "# Install Java (Spark requires JDK 8)\n",
        "!apt-get update -qq\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and extract Spark (includes Hadoop binaries for HDFS simulation)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark (helps Python locate Spark)\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables (mimics cluster config)\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "# Initialize findspark and create Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create Spark session in local mode: \"local[*]\" uses all cores for parallelism\n",
        "# appName: For job tracking (like in YARN on EMR)\n",
        "# Scaling note: In Dataproc/EMR, replace with .master(\"yarn\") and .config(\"spark.executor.memory\", \"4g\")\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"LinkedInJobPostingsIngestion\") \\\n",
        "    .config(\"spark.executor.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify setup\n",
        "print(\"Spark Version:\", spark.version)\n",
        "print(\"Hadoop Version (simulated):\", spark.conf.get(\"spark.hadoopVersion\", \"3.3.0\"))\n",
        "spark.sparkContext.setLogLevel(\"WARN\")  # Reduce log noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a803fe35",
        "outputId": "120b8ac8-5073-4c52-c3c6-e97b81be7e0f"
      },
      "source": [
        "# Replace the string 'NULL' with actual null values\n",
        "df = df.withColumn(\"job_level\", when(col(\"job_level\") == \"NULL\", None).otherwise(col(\"job_level\")))\n",
        "\n",
        "# Fill the null values with 'Unknown'\n",
        "df = df.na.fill({'job_level': 'Unknown'})\n",
        "\n",
        "# Show the unique values again to verify\n",
        "df.select(\"job_level\").distinct().show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|           job_level|\n",
            "+--------------------+\n",
            "|           Associate|\n",
            "|             Unknown|\n",
            "|          Mid senior|\n",
            "|Job Development S...|\n",
            "|Chief Computer Pr...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3ADqyjQKP91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e098362",
        "outputId": "fbdc4815-4006-4206-91cf-1c247ae3570e"
      },
      "source": [
        "df.select(\"job_level\").distinct().show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|           job_level|\n",
            "+--------------------+\n",
            "|           Associate|\n",
            "|          Mid senior|\n",
            "|Job Development S...|\n",
            "|Chief Computer Pr...|\n",
            "|                NULL|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b05dc46",
        "outputId": "a95402f9-9170-4f77-f6b0-85de473b3220"
      },
      "source": [
        "print(df.columns)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['job_link', 'last_processed_time', 'got_summary', 'got_ner', 'is_being_worked', 'job_title', 'company', 'job_location', 'first_seen', 'search_city', 'search_country', 'search_position', 'job_level', 'job_type']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "show count of jobs"
      ],
      "metadata": {
        "id": "WS31T6OSI5b_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show count of jobs\n",
        "df.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWw2Y-mEI6KC",
        "outputId": "eac9cd12-7ff5-4ee4-a7ff-e14c2ebda8b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "639433"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04600d32",
        "outputId": "25a22d36-21b3-4e48-ddda-a5c4189dde38"
      },
      "source": [
        "# Load the dataset\n",
        "# inferSchema: Spark attempts to automatically determine the data types of columns\n",
        "# header: Treat the first row as the header\n",
        "df = spark.read.csv(\"/content/linkedin_job_postings.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# Display the schema and a few rows to verify\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: string (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: string (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            "\n",
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
            "|            job_link| last_processed_time|got_summary|got_ner|is_being_worked|           job_title|             company|        job_location|first_seen|search_city|search_country|     search_position| job_level|job_type|\n",
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
            "|https://www.linke...|2024-01-21 07:12:...|          t|      t|              f|Account Executive...|                  BD|       San Diego, CA|2024-01-15|   Coronado| United States|         Color Maker|Mid senior|  Onsite|\n",
            "|https://www.linke...|2024-01-21 07:39:...|          t|      t|              f|Registered Nurse ...|   Trinity Health MI|   Norton Shores, MI|2024-01-14|Grand Haven| United States|Director Nursing ...|Mid senior|  Onsite|\n",
            "|https://www.linke...|2024-01-21 07:40:...|          t|      t|              f|RESTAURANT SUPERV...|Wasatch Adaptive ...|           Sandy, UT|2024-01-14|     Tooele| United States|            Stand-In|Mid senior|  Onsite|\n",
            "|https://www.linke...|2024-01-21 07:40:...|          t|      t|              f|Independent Real ...|Howard Hanna | Ra...|Englewood Cliffs, NJ|2024-01-16|  Pinehurst| United States|   Real-Estate Clerk|Mid senior|  Onsite|\n",
            "|https://www.linke...|2024-01-19 09:45:...|          f|      f|              f|Group/Unit Superv...|IRS, Office of Ch...|        Chamblee, GA|2024-01-17|    Gadsden| United States|Supervisor Travel...|Mid senior|  Onsite|\n",
            "+--------------------+--------------------+-----------+-------+---------------+--------------------+--------------------+--------------------+----------+-----------+--------------+--------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INGEST THE DATA INTO SPARK"
      ],
      "metadata": {
        "id": "Jbw153M5HhOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read CSV into Spark DataFrame\n",
        "# header=True: Uses first row as column names\n",
        "# inferSchema=True: Auto-detects data types (e.g., string for title, int for criteria_max_salary)\n",
        "# Options for distribution: Spark auto-partitions based on file size (e.g., 124K rows -> ~10-20 partitions)\n",
        "df = spark.read \\\n",
        "    .option(\"header\", \"True\") \\\n",
        "    .option(\"inferSchema\", \"True\") \\\n",
        "    .csv(\"linkedin_job_postings.csv\")\n",
        "\n",
        "# Cache for reuse (like in-memory distribution across nodes)\n",
        "df.cache()\n",
        "\n",
        "# Show schema (data types, nullable)\n",
        "df.printSchema()\n",
        "\n",
        "# Show first 5 rows and count\n",
        "print(\"Total rows:\", df.count())\n",
        "df.show(5, truncate=False)\n",
        "\n",
        "# Basic stats: Handle formats by checking for nulls/malformed rows\n",
        "df.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j2bjfsxHho2",
        "outputId": "60a25b5e-dec6-4669-f3fb-1358ddbff4fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- job_link: string (nullable = true)\n",
            " |-- last_processed_time: string (nullable = true)\n",
            " |-- got_summary: string (nullable = true)\n",
            " |-- got_ner: string (nullable = true)\n",
            " |-- is_being_worked: string (nullable = true)\n",
            " |-- job_title: string (nullable = true)\n",
            " |-- company: string (nullable = true)\n",
            " |-- job_location: string (nullable = true)\n",
            " |-- first_seen: string (nullable = true)\n",
            " |-- search_city: string (nullable = true)\n",
            " |-- search_country: string (nullable = true)\n",
            " |-- search_position: string (nullable = true)\n",
            " |-- job_level: string (nullable = true)\n",
            " |-- job_type: string (nullable = true)\n",
            "\n",
            "Total rows: 639433\n",
            "+------------------------------------------------------------------------------------------------------------------------------+-----------------------------+-----------+-------+---------------+--------------------------------------------------------------------------+----------------------------+--------------------+----------+-----------+--------------+------------------------------------+----------+--------+\n",
            "|job_link                                                                                                                      |last_processed_time          |got_summary|got_ner|is_being_worked|job_title                                                                 |company                     |job_location        |first_seen|search_city|search_country|search_position                     |job_level |job_type|\n",
            "+------------------------------------------------------------------------------------------------------------------------------+-----------------------------+-----------+-------+---------------+--------------------------------------------------------------------------+----------------------------+--------------------+----------+-----------+--------------+------------------------------------+----------+--------+\n",
            "|https://www.linkedin.com/jobs/view/account-executive-dispensing-norcal-northern-nevada-becton-dickinson-at-bd-3802078767      |2024-01-21 07:12:29.00256+00 |t          |t      |f              |Account Executive - Dispensing (NorCal/Northern Nevada) - Becton Dickinson|BD                          |San Diego, CA       |2024-01-15|Coronado   |United States |Color Maker                         |Mid senior|Onsite  |\n",
            "|https://www.linkedin.com/jobs/view/registered-nurse-rn-care-manager-at-trinity-health-mi-3803386312                           |2024-01-21 07:39:58.88137+00 |t          |t      |f              |Registered Nurse - RN Care Manager                                        |Trinity Health MI           |Norton Shores, MI   |2024-01-14|Grand Haven|United States |Director Nursing Service            |Mid senior|Onsite  |\n",
            "|https://www.linkedin.com/jobs/view/restaurant-supervisor-the-forklift-at-wasatch-adaptive-sports-3771464419                   |2024-01-21 07:40:00.251126+00|t          |t      |f              |RESTAURANT SUPERVISOR - THE FORKLIFT                                      |Wasatch Adaptive Sports     |Sandy, UT           |2024-01-14|Tooele     |United States |Stand-In                            |Mid senior|Onsite  |\n",
            "|https://www.linkedin.com/jobs/view/independent-real-estate-agent-at-howard-hanna-rand-realty-3797661348                       |2024-01-21 07:40:00.308133+00|t          |t      |f              |Independent Real Estate Agent                                             |Howard Hanna | Rand Realty  |Englewood Cliffs, NJ|2024-01-16|Pinehurst  |United States |Real-Estate Clerk                   |Mid senior|Onsite  |\n",
            "|https://www.linkedin.com/jobs/view/group-unit-supervisor-systems-support-manager-tss-at-irs-office-of-chief-counsel-3803057508|2024-01-19 09:45:09.215838+00|f          |f      |f              |Group/Unit Supervisor (Systems Support Manager TSS)                       |IRS, Office of Chief Counsel|Chamblee, GA        |2024-01-17|Gadsden    |United States |Supervisor Travel-Information Center|Mid senior|Onsite  |\n",
            "+------------------------------------------------------------------------------------------------------------------------------+-----------------------------+-----------+-------+---------------+--------------------------------------------------------------------------+----------------------------+--------------------+----------+-----------+--------------+------------------------------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+--------------------+--------------------+----------+---------------+--------------------+-----------------+--------------------+-------------+-----------+--------------+----------------+--------------------+---------+\n",
            "|summary|            job_link| last_processed_time|         got_summary|   got_ner|is_being_worked|           job_title|          company|        job_location|   first_seen|search_city|search_country| search_position|           job_level| job_type|\n",
            "+-------+--------------------+--------------------+--------------------+----------+---------------+--------------------+-----------------+--------------------+-------------+-----------+--------------+----------------+--------------------+---------+\n",
            "|  count|              639433|              639433|              639433|    639433|         639433|              639433|           639415|              639405|       639420|     639407|        639407|          639407|              639407|   639406|\n",
            "|   mean|                NULL|                NULL|                NULL|      NULL|           NULL|                NULL|            678.0|                NULL|         NULL|       NULL|          NULL|            NULL|                NULL|     NULL|\n",
            "| stddev|                NULL|                NULL|                NULL|      NULL|           NULL|                NULL|830.0038152522754|                NULL|         NULL|       NULL|          NULL|            NULL|                NULL|     NULL|\n",
            "|    min|\\t\\t\\t\\t\\t\\t\\t\\t\\...|2024-01-19 09:45:...|Borehamwood, Engl...|2024-01-12| Greater London|    \"\"\"Accountant\"\"\"|       $65/hr w2\"|            Inc.\"\")\"|   2024-01-12| 2024-01-12|    2024-01-13|     Able Seaman|Administrative As...|Associate|\n",
            "|    max|https://za.linked...|RED Engineering D...|                   t|         t|              t|🔥 Physician Prac...|  🚀 Creator Camp|Île-de-France, Fr...|Uniontown, OH|       Zion| United States|Zoo Veterinarian|              Onsite|   Remote|\n",
            "+-------+--------------------+--------------------+--------------------+----------+---------------+--------------------+-----------------+--------------------+-------------+-----------+--------------+----------------+--------------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VERIFY DATA DISTRIBUTION AND BASIC PROCESS"
      ],
      "metadata": {
        "id": "QnP_C5-cH5E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check partitions (mimics HDFS block distribution)\n",
        "print(\"Number of partitions:\", df.rdd.getNumPartitions())\n",
        "\n",
        "# Repartition for even distribution (e.g., scale to 4 \"nodes\")\n",
        "df_repartitioned = df.repartition(4)\n",
        "print(\"Repartitioned to:\", df_repartitioned.rdd.getNumPartitions())\n",
        "\n",
        "# Example: Filter and aggregate (distributed computation)\n",
        "# Group by company, count jobs (runs in parallel across partitions)\n",
        "top_companies = df.groupBy(\"company\").count().orderBy(col(\"count\").desc()).limit(10)\n",
        "top_companies.show()\n",
        "\n",
        "# Convert to Parquet for efficient storage (like HDFS output)\n",
        "df.write.mode(\"overwrite\").parquet(\"linkedin_jobs_parquet\")\n",
        "print(\"Saved as Parquet (compressed, distributed format)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSObwCQ2INT3",
        "outputId": "cc98957c-e3b3-4714-9931-a7a41a59bbe3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n",
            "Repartitioned to: 4\n",
            "+--------------------+-----+\n",
            "|             company|count|\n",
            "+--------------------+-----+\n",
            "|     Health eCareers|15268|\n",
            "|   Jobs for Humanity|12546|\n",
            "|      Dollar General| 6492|\n",
            "|   TravelNurseSource| 5731|\n",
            "|Gotham Enterprise...| 4310|\n",
            "|      Energy Jobline| 4290|\n",
            "|               Jobot| 4102|\n",
            "|      VolunteerMatch| 3469|\n",
            "|        PracticeLink| 3341|\n",
            "|       ClearanceJobs| 3304|\n",
            "+--------------------+-----+\n",
            "\n",
            "Saved as Parquet (compressed, distributed format)\n"
          ]
        }
      ]
    }
  ]
}